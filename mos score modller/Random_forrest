import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor

# ============================================================
# Load data
# ============================================================

script_dir = os.path.dirname(os.path.abspath(__file__))
data_path = os.path.join(script_dir, "df_files_2026-02-11_13-48-04.csv")
data = pd.read_csv(data_path)

# ============================================================
# Feature selection
# ============================================================

base_features = [
    "centroid_mean",
    "centroid_std",
    "bandwidth_mean",
    "bandwidth_std",
    "rolloff_mean",
    "rolloff_std",
    "flatness_mean",
    "flatness_std",

    "zcr_mean",
    "zcr_std",

    "rms_mean",
    "rms_std",

    "flux_mean",
    "flux_std",

    "contrast_mean",
    "contrast_std",

    "hnr",
]

# MFCC features (automatisk)
mfcc_features = []
for i in range(1, 14):
    mfcc_features.append(f"mfcc{i}_mean")
    mfcc_features.append(f"mfcc{i}_std")

all_features = base_features + mfcc_features

X = data[all_features]
y = data["mos"]

# ============================================================
# Train / Test split
# ============================================================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)

# ============================================================
# Model
# ============================================================

model = RandomForestRegressor(
    n_estimators=300,
    random_state=42,
    n_jobs=-1
)

# ============================================================
# Cross-validation
# ============================================================

cv_scores = cross_val_score(
    model,
    X,
    y,
    cv=5,
    scoring="neg_mean_absolute_error"
)

cv_mae = -cv_scores.mean()

print("\nCross-validation:")
print(f"MAE: {cv_mae:.3f}")

# ============================================================
# Training
# ============================================================

model.fit(X_train, y_train)

# ============================================================
# Predictions
# ============================================================

y_pred = model.predict(X_test)

# ============================================================
# Metrics
# ============================================================

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"\n{'='*40}")
print("Random Forest Regression RESULTS")
print(f"{'='*40}")
print(f"MAE:   {mae:.3f}")
print(f"RMSE:  {rmse:.3f}")

# ============================================================
# Plot: Prediction vs Ground Truth ⭐⭐⭐
# ============================================================

plt.figure()
plt.scatter(y_test, y_pred)
plt.xlabel("True MOS")
plt.ylabel("Predicted MOS")
plt.title("Prediction vs Ground Truth")
plt.plot([y.min(), y.max()], [y.min(), y.max()])
plt.show()

# ============================================================
# Feature Importance ⭐⭐⭐ (MEGET VIGTIG!)
# ============================================================

importances = model.feature_importances_

feature_importance_df = pd.DataFrame({
    "feature": all_features,
    "importance": importances
}).sort_values(by="importance", ascending=False)

print("\nTop 10 vigtigste features:")
print(feature_importance_df.head(10))

plt.figure()
plt.barh(
    feature_importance_df["feature"][:10],
    feature_importance_df["importance"][:10]
)
plt.gca().invert_yaxis()
plt.title("Top Feature Importances")
plt.show()